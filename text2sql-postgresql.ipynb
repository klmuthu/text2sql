{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# Advanced Text-to-SQL with PostgreSQL Vector Search\n\n## Production-Ready Hybrid Database Intelligence System\n\nThis notebook demonstrates the integration of **traditional relational database operations** with **vector search capabilities** in PostgreSQL, featuring automated query strategy selection based on user intent analysis.\n\n### üéØ **Core Technical Demonstrations:**\n\n#### 1. **Complex Schema Text-to-SQL Generation**\n- LLM-powered natural language to SQL conversion across multi-table schemas\n- Handling hierarchical data structures, complex joins, and nested aggregations\n- Demonstrating schema comprehension for enterprise-scale database architectures\n\n#### 2. **PostgreSQL pgvector Integration** \n- Native vector storage and similarity search within PostgreSQL\n- Embedding-based semantic search on unstructured text data\n- Demonstrating RDBMS + vector database convergence in production environments\n\n#### 3. **Automated Query Strategy Selection**\n- Foundation model analysis of query intent and optimal execution path determination\n- Context-aware routing between structured SQL and semantic vector operations\n- Unified interface abstracting query complexity from end users\n\n### üèóÔ∏è **Database Schema Architecture**\n\nProduction-ready ecommerce schema demonstrating complex relationships:\n\n```\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     users       ‚îÇ    ‚îÇ    categories    ‚îÇ    ‚îÇ    products     ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ user_id (PK)    ‚îÇ    ‚îÇ category_id (PK) ‚îÇ    ‚îÇ product_id (PK) ‚îÇ\n‚îÇ email           ‚îÇ    ‚îÇ name             ‚îÇ    ‚îÇ sku             ‚îÇ\n‚îÇ username        ‚îÇ    ‚îÇ slug             ‚îÇ    ‚îÇ name            ‚îÇ\n‚îÇ first_name      ‚îÇ    ‚îÇ description      ‚îÇ    ‚îÇ description     ‚îÇ\n‚îÇ last_name       ‚îÇ    ‚îÇ parent_category_id‚îÇ   ‚îÇ category_id (FK)‚îÇ\n‚îÇ city            ‚îÇ    ‚îÇ   (FK to self)   ‚îÇ    ‚îÇ brand           ‚îÇ\n‚îÇ state_province  ‚îÇ    ‚îÇ product_count    ‚îÇ    ‚îÇ price           ‚îÇ\n‚îÇ total_orders    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ stock_quantity  ‚îÇ\n‚îÇ total_spent     ‚îÇ           ‚îÇ                ‚îÇ rating_average  ‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ                ‚îÇ total_sales     ‚îÇ\n         ‚îÇ                    ‚îÇ                ‚îÇ revenue_generated‚îÇ\n         ‚îÇ                    ‚îÇ                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n         ‚îÇ                    ‚îÇ                         ‚îÇ\n         ‚îÇ                    ‚îÇ                         ‚îÇ\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n‚îÇ     orders      ‚îÇ    ‚îÇ   order_items    ‚îÇ    ‚îÇ    reviews      ‚îÇ\n‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n‚îÇ order_id (PK)   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ order_id (FK)    ‚îÇ    ‚îÇ review_id (PK)  ‚îÇ\n‚îÇ order_number    ‚îÇ    ‚îÇ product_id (FK)  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ product_id (FK) ‚îÇ\n‚îÇ user_id (FK)    ‚îÇ    ‚îÇ quantity         ‚îÇ    ‚îÇ user_id (FK)    ‚îÇ\n‚îÇ order_status    ‚îÇ    ‚îÇ unit_price       ‚îÇ    ‚îÇ order_id (FK)   ‚îÇ\n‚îÇ payment_status  ‚îÇ    ‚îÇ total_price      ‚îÇ    ‚îÇ rating          ‚îÇ\n‚îÇ total_amount    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ title           ‚îÇ\n‚îÇ shipping_method ‚îÇ                            ‚îÇ comment         ‚îÇ\n‚îÇ created_at      ‚îÇ                            ‚îÇ comment_embedding‚îÇ\n‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                            ‚îÇ   (VECTOR)      ‚îÇ\n                                               ‚îÇ pros            ‚îÇ\n                                               ‚îÇ cons            ‚îÇ\n                                               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n```\n\n**Schema Complexity Features:**\n- **Self-referencing hierarchies**: Categories with parent/child relationships\n- **Junction table patterns**: Many-to-many order-product relationships via order_items\n- **Vector integration**: Native pgvector storage in reviews.comment_embedding\n- **Denormalized analytics**: Pre-computed aggregations for performance optimization\n- **Multi-level foreign keys**: Reviews referencing users, products, and orders\n\n### üí° **Technical Implementation:**\n\n1. **Hybrid Database Architecture**: PostgreSQL with pgvector extension for unified structured + vector operations\n2. **LLM Schema Comprehension**: Foundation model understanding of complex table relationships and optimal query generation\n3. **Embedding-based Similarity**: Amazon Titan text embeddings for semantic content matching\n4. **Automated Tool Selection**: Context analysis determining SQL vs vector search execution paths\n\n## Technical Prerequisites\n- AWS account with Bedrock and RDS permissions\n- Understanding of vector embeddings and similarity search concepts\n- Familiarity with PostgreSQL and complex SQL operations\n\n---",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## üì¶ STEP 1: Install Required Packages",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0afb548c-60e7-4929-9ae2-26113df7e5b0",
   "metadata": {},
   "outputs": [],
   "source": "# Install required Python packages for AWS and SQL parsing\n!pip install --upgrade pip\n!pip install boto3 sqlparse"
  },
  {
   "cell_type": "markdown",
   "source": "## üèóÔ∏è STEP 2: Deploy AWS Infrastructure\n\nThis step creates:\n- **VPC with 3 subnets** across availability zones\n- **Aurora PostgreSQL Serverless v2 cluster** with HTTP endpoint enabled\n- **Security groups** and networking configuration\n- **Secrets Manager** for database credentials\n\n**Note**: This takes ~5-10 minutes to complete",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "4efd1122-9bee-4cde-a536-0d149b796f0f",
   "metadata": {},
   "outputs": [],
   "source": "# Deploy AWS infrastructure (VPC, Aurora PostgreSQL, Security Groups)\n# This script creates all necessary AWS resources for our demo\n\n# Option 1: Use subprocess (recommended)\nimport subprocess\nimport sys\n\ntry:\n    print(\"üöÄ Starting AWS infrastructure deployment...\")\n    result = subprocess.run([sys.executable, 'infra.py'], \n                          capture_output=False, \n                          text=True, \n                          check=True)\n    print(\"‚úÖ Infrastructure deployment completed successfully!\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"‚ùå Infrastructure deployment failed: {e}\")\nexcept FileNotFoundError:\n    print(\"‚ùå infra.py not found. Please ensure the file exists in the current directory.\")\n    print(\"üí° You can run this manually in terminal: python infra.py\")\n\n# Option 2: Use magic command (alternative)\n# %run infra.py"
  },
  {
   "cell_type": "markdown",
   "source": "## üîß STEP 3: Setup Database Connection",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "6a29e0a2-12ff-42f7-b1aa-8ffecf097633",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries for AWS services and database operations\nimport json\nimport boto3\nimport logging\nimport sqlparse\nfrom typing import Dict, Any, List, Union\nfrom botocore.exceptions import ClientError\nfrom botocore.config import Config\n\n# Setup logging to track our progress\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")"
  },
  {
   "cell_type": "code",
   "id": "1bb84fc4-0b36-493d-a269-60959ece77a9",
   "metadata": {},
   "outputs": [],
   "source": "# Database connection configuration\n# These values match what was created by infra.py\nCLUSTER_ARN = 'arn:aws:rds:us-west-2:831993209541:cluster:aurora-text2sql-cluster'\nSECRET_ARN = 'arn:aws:secretsmanager:us-west-2:831993209541:secret:rds!cluster-be97ab95-23e3-4eed-bacf-3b56ccf5ce33-56lAgn'\nDATABASE_NAME = 'ecommerce'  # We'll create this database for our demo\n\n# Initialize RDS Data API client (allows SQL execution without direct connections)\nrds_client = boto3.client('rds-data')"
  },
  {
   "cell_type": "markdown",
   "source": "## üõ†Ô∏è STEP 4: Create Database Schema & Load Data\n\nWe'll create a streamlined but complex ecommerce schema with 6 core tables that demonstrate:\n- **Hierarchical relationships** (categories with parent/child structure)\n- **Many-to-many relationships** (orders ‚Üî products via junction table) \n- **Vector integration** (reviews with embedding column for semantic search)\n- **Analytics capabilities** (aggregated sales metrics and customer data)",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bd1d2ddd-b839-4fe0-9fb9-edf566807245",
   "metadata": {},
   "outputs": [],
   "source": "def run_sql(query: str, database: str = None) -> dict:\n    \"\"\"\n    Execute SQL query using RDS Data API\n    This is our main function for running any SQL command\n    \"\"\"\n    try:\n        params = {\n            'resourceArn': CLUSTER_ARN,\n            'secretArn': SECRET_ARN,\n            'sql': query\n        }\n        if database:\n            params['database'] = database\n            \n        response = rds_client.execute_statement(**params)\n        return response\n    except Exception as e:\n        print(f\"SQL execution error: {e}\")\n        return {\"error\": str(e)}\n\n# Create our ecommerce database\ntry:\n    run_sql(f'CREATE DATABASE {DATABASE_NAME};')\n    print(f\"‚úÖ Database '{DATABASE_NAME}' created successfully\")\nexcept Exception as e:\n    print(f\"Database may already exist: {e}\")"
  },
  {
   "cell_type": "code",
   "id": "440ad441-0550-49c5-af88-15efec549268",
   "metadata": {},
   "outputs": [],
   "source": "# Enable pgvector extension for semantic search capabilities\n# pgvector allows PostgreSQL to store and search vector embeddings\ntry:\n    result = run_sql('CREATE EXTENSION IF NOT EXISTS vector;', DATABASE_NAME)\n    print(\"‚úÖ pgvector extension enabled successfully\")\nexcept Exception as e:\n    print(f\"Extension setup error: {e}\")"
  },
  {
   "cell_type": "code",
   "id": "2d52f3db-5f84-44d1-9292-470afdc863a1",
   "metadata": {},
   "outputs": [],
   "source": "# Create tables by reading our streamlined schema file\n# Parse SQL file into individual statements (RDS Data API requirement)\nwith open('ecommerce_schema.sql', 'r') as f:\n    schema_sql = f.read()\n\nstatements = sqlparse.split(schema_sql)\nstatements = [stmt.strip() for stmt in statements if stmt.strip()]\n\nprint(f\"Creating {len(statements)} database tables...\")\nprint(\"üìä Schema includes: users, categories, products, orders, order_items, reviews\")\nprint(\"üîó Complex relationships: hierarchical categories, order-product junction table\")\nprint(\"üß† Vector integration: reviews.comment_embedding for semantic search\")\n\n# Execute each CREATE TABLE statement\nfor i, statement in enumerate(statements, 1):\n    try:\n        run_sql(statement, DATABASE_NAME)\n        print(f\"  ‚úÖ Table {i} created successfully\")\n    except Exception as e:\n        print(f\"  ‚ùå Table {i} failed: {e}\")\n\nprint(\"‚úÖ Database schema creation completed!\")"
  },
  {
   "cell_type": "code",
   "id": "19105a1b-ae33-4619-9860-88d2c9a93be8",
   "metadata": {},
   "outputs": [],
   "source": "# Insert comprehensive sample data into our tables\nwith open('ecommerce_data.sql', 'r') as f:\n    data_sql = f.read()\n\ndata_statements = sqlparse.split(data_sql)\ndata_statements = [stmt.strip() for stmt in data_statements if stmt.strip()]\n\nprint(f\"Inserting sample data with {len(data_statements)} statements...\")\nprint(\"üë• 15 users across different US cities with spending history\")\nprint(\"üì¶ 16 products across 8 categories (Electronics ‚Üí Audio/Video, Smart Devices, etc.)\")\nprint(\"üõí 10 orders with various statuses (delivered, shipped, processing, cancelled)\")\nprint(\"‚≠ê 13 detailed product reviews perfect for semantic search\")\n\nfor i, statement in enumerate(data_statements, 1):\n    try:\n        result = run_sql(statement, DATABASE_NAME)\n        records_affected = result.get('numberOfRecordsUpdated', 0)\n        print(f\"  ‚úÖ Dataset {i}: {records_affected} records inserted\")\n    except Exception as e:\n        print(f\"  ‚ùå Dataset {i} failed: {e}\")\n\nprint(\"‚úÖ Sample data insertion completed!\")\nprint(\"üéØ Ready for complex SQL queries and semantic search demonstrations\")"
  },
  {
   "cell_type": "markdown",
   "source": "## üß† STEP 5: Setup AI Models (Bedrock)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "cfcd2744-65c2-4bf4-87e5-ef21e781bdc5",
   "metadata": {},
   "outputs": [],
   "source": "# Setup Amazon Bedrock for AI capabilities\n# Claude Sonnet 4.0 for intelligent text-to-SQL conversion\n# Titan Embeddings for vector generation\n\n# Configure Bedrock client with extended timeouts for large requests\nbedrock_config = Config(\n    connect_timeout=60*5,  # 5 minutes\n    read_timeout=60*5,     # 5 minutes\n)\n\nbedrock = boto3.client(\n    service_name='bedrock-runtime', \n    region_name='us-west-2', \n    config=bedrock_config\n)\n\n# Model IDs for our AI services\nCLAUDE_MODEL = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"  # For text-to-SQL\nEMBEDDING_MODEL = \"amazon.titan-embed-text-v2:0\"                # For vector search\n\nprint(\"‚úÖ Bedrock AI models configured successfully\")"
  },
  {
   "cell_type": "code",
   "id": "5352dc09-f513-4c27-b619-376dfcf49d60",
   "metadata": {},
   "outputs": [],
   "source": "class DatabaseTools:\n    \"\"\"Simple database helper for executing SQL queries\"\"\"\n    \n    def __init__(self):\n        self.rds_client = boto3.client(\"rds-data\", region_name='us-west-2')\n    \n    def execute_sql(self, query: str) -> str:\n        \"\"\"Execute SQL query and return results as JSON string\"\"\"\n        try:\n            response = self.rds_client.execute_statement(\n                resourceArn=CLUSTER_ARN,\n                secretArn=SECRET_ARN,\n                database=DATABASE_NAME,\n                sql=query,\n                includeResultMetadata=True,\n            )\n            \n            # Handle empty results\n            if \"records\" not in response or not response[\"records\"]:\n                return json.dumps([])\n            \n            # Get column names and format results\n            columns = [field[\"name\"] for field in response.get(\"columnMetadata\", [])]\n            results = []\n            \n            for record in response[\"records\"]:\n                row_values = []\n                for field in record:\n                    # Extract value from different field types\n                    if \"stringValue\" in field:\n                        row_values.append(field[\"stringValue\"])\n                    elif \"longValue\" in field:\n                        row_values.append(field[\"longValue\"])\n                    elif \"doubleValue\" in field:\n                        row_values.append(field[\"doubleValue\"])\n                    elif \"booleanValue\" in field:\n                        row_values.append(field[\"booleanValue\"])\n                    else:\n                        row_values.append(None)\n                \n                results.append(dict(zip(columns, row_values)))\n            \n            return json.dumps(results, indent=2)\n            \n        except Exception as e:\n            return json.dumps({\"error\": f\"Database error: {str(e)}\"})\n\n# Test database connection\ndb_tools = DatabaseTools()\nresult = db_tools.execute_sql(\"SELECT current_timestamp;\")\nprint(\"‚úÖ Database connection test successful\")\nprint(\"Current time:\", json.loads(result)[0][\"current_timestamp\"])"
  },
  {
   "cell_type": "markdown",
   "source": "## üî¢ STEP 6: Generate Vector Embeddings for Semantic Search\n\n**Hybrid RDBMS + Vector Database Implementation:**\n\nVector embeddings convert textual content into high-dimensional numerical representations that capture semantic relationships. PostgreSQL's pgvector extension enables native vector operations within the relational database, eliminating the need for separate vector database infrastructure.\n\n**Technical Implementation:**\n- Amazon Titan Text Embeddings v2 (1024-dimensional vectors)\n- PostgreSQL VECTOR data type with cosine similarity operations\n- Semantic search on review content independent of exact keyword matching\n\nThis approach demonstrates the convergence of traditional RDBMS and vector database capabilities in production systems.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "18a417e1-8051-40b0-84c0-464f9f7923ae",
   "metadata": {},
   "outputs": [],
   "source": "def create_embedding(text: str) -> List[float]:\n    \"\"\"\n    Convert text into a vector embedding using Amazon Titan\n    Returns a list of 1024 numbers that represent the text's meaning\n    \"\"\"\n    payload = {\n        \"inputText\": text,\n        \"embeddingTypes\": [\"float\"]\n    }\n    \n    try:\n        response = bedrock.invoke_model(\n            modelId=EMBEDDING_MODEL,\n            body=json.dumps(payload),\n            accept=\"application/json\",\n            contentType=\"application/json\"\n        )\n        \n        body = json.loads(response[\"body\"].read())\n        embeddings = body.get(\"embeddingsByType\", {}).get(\"float\", [])\n        return embeddings\n        \n    except Exception as e:\n        print(f\"Embedding generation error: {e}\")\n        return []\n\n# Test embedding generation\ntest_text = \"This battery lasts a long time\"\ntest_embedding = create_embedding(test_text)\nprint(f\"‚úÖ Generated embedding with {len(test_embedding)} dimensions\")\nprint(f\"Sample values: {test_embedding[:5]}...\")  # Show first 5 numbers"
  },
  {
   "cell_type": "code",
   "id": "a2a95f6d-6ea7-418d-9930-c50b854ca0c5",
   "metadata": {},
   "outputs": [],
   "source": "def add_embeddings_to_reviews():\n    \"\"\"\n    Generate embeddings for all review comments and store them in the database\n    This enables semantic search on review content\n    \"\"\"\n    \n    # Step 1: Find reviews that need embeddings\n    count_query = \"SELECT COUNT(*) FROM reviews WHERE comment_embedding IS NULL\"\n    count_result = db_tools.execute_sql(count_query)\n    total_missing = json.loads(count_result)[0][\"count\"]\n    \n    print(f\"Found {total_missing} reviews needing embeddings\")\n    \n    if total_missing == 0:\n        print(\"‚úÖ All reviews already have embeddings!\")\n        return\n    \n    # Step 2: Get reviews without embeddings\n    select_query = \"\"\"\n        SELECT review_id, comment \n        FROM reviews \n        WHERE comment_embedding IS NULL \n        AND comment IS NOT NULL\n        ORDER BY review_id\n    \"\"\"\n    \n    result = db_tools.execute_sql(select_query)\n    reviews = json.loads(result)\n    \n    # Step 3: Generate embeddings for each review\n    for review in reviews:\n        review_id = review[\"review_id\"]\n        comment = review[\"comment\"]\n        \n        if not comment:\n            continue\n            \n        print(f\"  Processing review {review_id}...\")\n        \n        # Generate embedding\n        embedding = create_embedding(comment)\n        if not embedding:\n            continue\n            \n        # Convert to PostgreSQL vector format\n        vector_str = \"[\" + \",\".join(str(x) for x in embedding) + \"]\"\n        \n        # Update database with embedding\n        update_query = f\"\"\"\n            UPDATE reviews \n            SET comment_embedding = '{vector_str}'::vector \n            WHERE review_id = {review_id}\n        \"\"\"\n        \n        run_sql(update_query, DATABASE_NAME)\n        print(f\"    ‚úÖ Added embedding for review {review_id}\")\n    \n    print(\"‚úÖ All review embeddings generated successfully!\")\n\n# Generate embeddings for all reviews\nadd_embeddings_to_reviews()"
  },
  {
   "cell_type": "markdown",
   "source": "## ü§ñ STEP 7: Foundation Model Tool Selection System\n\n**Query Strategy Determination:**\n\nClaude Sonnet 4.0 analyzes natural language queries and automatically determines the optimal execution strategy through tool selection logic:\n\n**üìä Structured Query Scenarios (SQL Tool Selection):**\n- Aggregation operations: \"What's the average order value by state?\"\n- Complex joins: \"Show customers with repeat purchases in Electronics\"\n- Mathematical calculations: \"Calculate profit margins by product category\"\n- Temporal analysis: \"Find order trends over the last quarter\"\n\n**üîç Semantic Search Scenarios (Vector Tool Selection):**\n- Content similarity: \"Find reviews about build quality issues\"\n- Sentiment analysis: \"Show complaints about customer service\"\n- Topic clustering: \"What do users say about product durability?\"\n- Conceptual matching: Independent of exact keyword presence\n\n**üéØ Hybrid Query Execution:**\n- Complex scenarios may trigger multiple tool usage\n- Foundation model orchestrates sequential or parallel execution\n- Results synthesis from both structured and semantic operations\n\n**Technical Architecture:**\n- Tool specification via JSON schema definitions\n- Automated function calling based on intent classification\n- Context-aware execution path optimization",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "cf4ee1a5-c963-48d3-bbb7-efbde659aac1",
   "metadata": {},
   "outputs": [],
   "source": "def semantic_search(search_text: str, limit: int = 5) -> str:\n    \"\"\"\n    Find reviews similar to the search text using vector similarity\n    Returns the most semantically similar reviews\n    \"\"\"\n    try:\n        # Generate embedding for search text\n        search_embedding = create_embedding(search_text)\n        if not search_embedding:\n            return json.dumps({\"error\": \"Could not generate embedding\"})\n        \n        # Convert to PostgreSQL vector format\n        vector_str = \"[\" + \",\".join(str(x) for x in search_embedding) + \"]\"\n        \n        # Find similar reviews using cosine distance (<-> operator)\n        query = f\"\"\"\n        SELECT \n            rating,\n            title,\n            comment,\n            pros,\n            cons,\n            helpful_count,\n            (1 - (comment_embedding <-> '{vector_str}'::vector)) as similarity_score\n        FROM reviews\n        WHERE comment IS NOT NULL \n        AND comment_embedding IS NOT NULL\n        ORDER BY comment_embedding <-> '{vector_str}'::vector\n        LIMIT {limit}\n        \"\"\"\n        \n        result = db_tools.execute_sql(query)\n        return result\n        \n    except Exception as e:\n        return json.dumps({\"error\": f\"Vector search error: {str(e)}\"})\n\n# Test vector search\ntest_search = semantic_search(\"battery problems\", limit=3)\nprint(\"‚úÖ Vector search test successful\")\nprint(\"Sample results:\", json.loads(test_search)[0][\"title\"] if json.loads(test_search) else \"No results\")"
  },
  {
   "cell_type": "code",
   "id": "1a5c586e-b60f-4283-a0be-8a4876e22de0",
   "metadata": {},
   "outputs": [],
   "source": "# Define the tools available to Claude\nAI_TOOLS = {\n    \"tools\": [\n        {\n            \"toolSpec\": {\n                \"name\": \"execute_sql\",\n                \"description\": \"Execute SQL queries for structured data analysis (counts, filters, joins, aggregations)\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"query\": {\n                                \"type\": \"string\",\n                                \"description\": \"SQL query to execute against the ecommerce database\"\n                            }\n                        },\n                        \"required\": [\"query\"]\n                    }\n                }\n            }\n        },\n        {\n            \"toolSpec\": {\n                \"name\": \"vector_search\",\n                \"description\": \"Perform semantic similarity search on review content to find similar topics/themes\",\n                \"inputSchema\": {\n                    \"json\": {\n                        \"type\": \"object\",\n                        \"properties\": {\n                            \"text\": {\n                                \"type\": \"string\",\n                                \"description\": \"Text to search for semantically similar content in reviews\"\n                            }\n                        },\n                        \"required\": [\"text\"]\n                    }\n                }\n            }\n        }\n    ],\n    \"toolChoice\": {\"auto\": {}}\n}\n\nprint(\"‚úÖ AI tools configured - Claude can now choose between SQL and vector search!\")"
  },
  {
   "cell_type": "code",
   "id": "9420c5ac-1ba4-487d-a6c5-0dab2b32f8c0",
   "metadata": {},
   "outputs": [],
   "source": "# Foundation model system prompt with detailed schema specification\nSYSTEM_PROMPT = \"\"\"\nYou are a database query optimization system with automated tool selection for hybrid SQL and vector search operations.\n\nSCHEMA SPECIFICATION (6-table ecommerce system):\n\n**users** (customer analytics):\n- user_id, email, username, first_name, last_name, city, state_province\n- total_orders, total_spent (denormalized aggregations)\n\n**categories** (hierarchical taxonomy):\n- category_id, name, slug, description, parent_category_id (self-referencing FK)\n- product_count (computed metric)\n\n**products** (catalog with analytics):\n- product_id, sku, name, description, category_id (FK to categories)\n- brand, price, cost, stock_quantity, weight_kg\n- rating_average, rating_count, total_sales, revenue_generated (computed metrics)\n\n**orders** (transaction management):\n- order_id, order_number, user_id (FK to users)\n- order_status, payment_status, shipping_address, payment_method, shipping_method\n- subtotal, tax_amount, shipping_cost, discount_amount, total_amount\n- shipped_at, delivered_at, created_at\n\n**order_items** (junction table):\n- order_item_id, order_id (FK), product_id (FK)\n- quantity, unit_price, discount_amount, tax_amount, total_price\n\n**reviews** (with vector search capability):\n- review_id, product_id (FK), user_id (FK), order_id (FK)\n- rating, title, comment, comment_embedding (VECTOR(1024) for semantic search)\n- pros, cons, is_verified_purchase, helpful_count\n\nRELATIONSHIP PATTERNS:\n- Self-referencing: categories.parent_category_id\n- One-to-many: users‚Üíorders, products‚Üíreviews, categories‚Üíproducts\n- Many-to-many: orders‚Üîproducts via order_items junction\n- Multi-reference: reviews link to users, products, and orders\n\nTOOL SELECTION LOGIC:\n1. execute_sql for:\n   - Quantitative analysis, aggregations, mathematical operations\n   - Multi-table joins, hierarchical queries, temporal analysis\n   - Structured data filtering, sorting, grouping operations\n   - Performance metrics, business intelligence queries\n\n2. vector_search for:\n   - Semantic similarity operations on reviews.comment field\n   - Content-based analysis independent of exact keywords\n   - Sentiment and topic analysis of unstructured text\n   - Conceptual matching and thematic clustering\n\n3. Multi-tool execution for complex analytical requirements\n\nOUTPUT REQUIREMENTS:\n1. Specify tool selection rationale\n2. Present results with technical accuracy\n3. Identify relevant patterns and insights in the data\n\"\"\"\n\nprint(\"‚úÖ Foundation model configured with comprehensive schema specification\")\nprint(\"üîß Tool selection logic optimized for hybrid query execution\")"
  },
  {
   "cell_type": "code",
   "id": "542b7cc1-da90-4d63-912b-2ba8b5f97488",
   "metadata": {},
   "outputs": [],
   "source": "def ask_ai(question: str) -> str:\n    \"\"\"\n    Send a question to Claude Sonnet 4.0 and handle tool execution\n    Claude will automatically choose between SQL and vector search\n    \"\"\"\n    \n    # Create the conversation\n    messages = [{\"role\": \"user\", \"content\": [{\"text\": question}]}]\n    \n    try:\n        # Send to Claude with tools\n        response = bedrock.converse(\n            modelId=CLAUDE_MODEL,\n            system=[{\"text\": SYSTEM_PROMPT}],\n            messages=messages,\n            toolConfig=AI_TOOLS\n        )\n        \n        assistant_message = response[\"output\"][\"message\"]\n        messages.append(assistant_message)\n        \n        # Check if Claude wants to use tools\n        tool_uses = [content for content in assistant_message[\"content\"] if \"toolUse\" in content]\n        \n        if tool_uses:\n            # Execute each tool Claude requested\n            for tool_use in tool_uses:\n                tool_name = tool_use[\"toolUse\"][\"name\"]\n                tool_input = tool_use[\"toolUse\"][\"input\"]\n                tool_id = tool_use[\"toolUse\"][\"toolUseId\"]\n                \n                print(f\"üîß Claude is using: {tool_name}\")\n                \n                # Execute the appropriate tool\n                if tool_name == \"execute_sql\":\n                    tool_result = db_tools.execute_sql(tool_input[\"query\"])\n                    print(f\"üìä SQL Query: {tool_input['query']}\")\n                    \n                elif tool_name == \"vector_search\":\n                    tool_result = semantic_search(tool_input[\"text\"])\n                    print(f\"üîç Searching for: {tool_input['text']}\")\n                \n                # Send tool result back to Claude\n                tool_message = {\n                    \"role\": \"user\",\n                    \"content\": [{\n                        \"toolResult\": {\n                            \"toolUseId\": tool_id,\n                            \"content\": [{\"text\": tool_result}]\n                        }\n                    }]\n                }\n                messages.append(tool_message)\n            \n            # Get Claude's final response after tool execution\n            final_response = bedrock.converse(\n                modelId=CLAUDE_MODEL,\n                system=[{\"text\": SYSTEM_PROMPT}],\n                messages=messages,\n                toolConfig=AI_TOOLS\n            )\n            \n            # Extract the text response\n            final_content = final_response[\"output\"][\"message\"][\"content\"]\n            return next(c[\"text\"] for c in final_content if \"text\" in c)\n        \n        else:\n            # No tools needed, return direct response\n            return next(c[\"text\"] for c in assistant_message[\"content\"] if \"text\" in c)\n            \n    except Exception as e:\n        return f\"Error: {str(e)}\"\n\nprint(\"‚úÖ AI assistant ready! You can now ask questions in natural language.\")"
  },
  {
   "cell_type": "markdown",
   "source": "## üöÄ STEP 8: Technical Demonstrations\n\n### Demo 1: Complex Schema Text-to-SQL Generation\n**Objective:** Validate LLM comprehension of multi-table relationships and automated SQL generation for complex analytical queries.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "8ca62e41-6c03-49c8-a280-426d9de633c6",
   "metadata": {},
   "outputs": [],
   "source": "# DEMO 1: Complex Schema Text-to-SQL Generation\nprint(\"=\" * 70)\nprint(\"DEMO 1: Complex Schema Text-to-SQL Generation\")\nprint(\"=\" * 70)\n\n# Test multi-table join with hierarchical traversal and aggregation\nquestion1 = \"Show me the top 3 customers by total spending, including their order count and favorite product category\"\nprint(f\"Query: {question1}\")\nprint(\"\\nüîß Expected: Multi-table JOIN across users, orders, order_items, products, categories\")\nprint(\"üìä Complexity: Aggregation + hierarchical category resolution + ranking\")\nprint(\"\\nExecution:\")\nanswer1 = ask_ai(question1)\nprint(answer1)\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "source": "### Demo 2: PostgreSQL Vector Search Implementation\n**Objective:** Demonstrate native vector operations within PostgreSQL using pgvector for semantic similarity search on unstructured content.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "47633120-d09e-47c6-a0ca-c8973eb70463",
   "metadata": {},
   "outputs": [],
   "source": "# DEMO 2: PostgreSQL Vector Search Implementation\nprint(\"DEMO 2: PostgreSQL Vector Search Implementation\")\nprint(\"=\" * 70)\n\nquestion2 = \"Find reviews about battery life issues and charging problems\"\nprint(f\"Query: {question2}\")\nprint(\"\\nüîß Expected: Vector similarity search using pgvector cosine distance\")\nprint(\"üìä Operation: Embedding generation + semantic matching on reviews.comment_embedding\")\nprint(\"üéØ Capability: Content similarity independent of exact keyword presence\")\nprint(\"\\nExecution:\")\nanswer2 = ask_ai(question2)\nprint(answer2)\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "source": "### Demo 3: Automated Query Strategy Selection\n**Objective:** Validate foundation model capability to analyze query intent and select optimal execution strategy between SQL and vector operations.",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "12c86224-f7ff-40e6-92f7-9da4d520c895",
   "metadata": {},
   "outputs": [],
   "source": "# DEMO 3: Automated Query Strategy Selection\nprint(\"DEMO 3: Automated Query Strategy Selection\")\nprint(\"=\" * 70)\n\n# Ambiguous query that could use either approach\nquestion3 = \"What are the main product quality issues customers mention in their reviews?\"\nprint(f\"Query: {question3}\")\nprint(\"\\nü§î Strategy Options:\")\nprint(\"   üìä SQL Approach: Aggregate review ratings and identify low-rated products\")\nprint(\"   üîç Vector Approach: Semantic search for quality-related content themes\") \nprint(\"   üéØ Hybrid Approach: Combine structured filtering with content analysis\")\nprint(\"\\nüîß Foundation Model Decision Process:\")\nprint(\"\\nExecution:\")\nanswer3 = ask_ai(question3)\nprint(answer3)\nprint(\"\\n\" + \"=\"*70)"
  },
  {
   "cell_type": "markdown",
   "source": "## üí¨ Interactive Query Testing\n\n**Technical Validation Environment**\n\nTest the foundation model's query strategy selection across different analytical scenarios. The system will demonstrate automated tool selection based on query characteristics and optimal execution path determination.\n\n**Structured Query Test Cases:**\n\n**üìä Complex SQL Operations:**\n- \"Calculate profit margins by hierarchical product category\"\n- \"Identify customers with highest purchase frequency in Texas\"\n- \"Analyze order value distribution across payment methods\"\n- \"Show temporal trends in Electronics subcategory sales\"\n\n**üîç Vector Similarity Operations:**\n- \"Find reviews discussing build quality and manufacturing defects\"\n- \"Locate customer feedback about shipping and logistics issues\"\n- \"Identify content related to product longevity and durability concerns\"\n- \"Search for mentions of value proposition and pricing feedback\"\n\n**üéØ Complex Analytical Scenarios:**\n- \"Which products receive the most quality-related complaints?\"\n- \"Analyze sentiment patterns across different customer segments\"\n- \"Find correlation between product price points and satisfaction themes\"",
   "metadata": {},
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "92781fad-9afc-469d-aec7-c693da3b2fe6",
   "metadata": {},
   "outputs": [],
   "source": "# Interactive Query Testing Environment\nprint(\"üîß Foundation Model Query Strategy Testing\")\nprint(\"Enter queries to validate automated tool selection logic. Type 'quit' to exit.\")\nprint(\"\\nüìã Test Categories:\")\n\nprint(\"\\nüìä Structured Data Operations:\")\nprint(\"‚Ä¢ 'Which product categories have the highest profit margins?'\")\nprint(\"‚Ä¢ 'Show customer geographic distribution by total spending'\")\nprint(\"‚Ä¢ 'Analyze order completion rates by shipping method'\")\n\nprint(\"\\nüîç Semantic Content Analysis:\")\nprint(\"‚Ä¢ 'Find reviews about products being difficult to use or setup'\")\nprint(\"‚Ä¢ 'Locate feedback about customer support experiences'\")\nprint(\"‚Ä¢ 'Search for mentions of product packaging and presentation'\")\n\nprint(\"\\nüéØ Hybrid Analysis Scenarios:\")\nprint(\"‚Ä¢ 'Identify top-selling products with usability complaints'\")\nprint(\"‚Ä¢ 'Find high-value customers who mention quality concerns'\")\n\nprint(\"-\" * 70)\n\nwhile True:\n    question = input(\"\\nüîç Query: \").strip()\n    \n    if question.lower() == 'quit':\n        print(\"‚úÖ Query testing session completed\")\n        break\n    \n    if question:\n        print(f\"\\nüìù Processing: {question}\")\n        print(\"‚öôÔ∏è  Analyzing query intent and determining execution strategy...\")\n        answer = ask_ai(question)\n        print(f\"\\nüìä Result: {answer}\")\n        print(\"-\" * 70)"
  },
  {
   "cell_type": "markdown",
   "source": "## üßπ STEP 9: Cleanup (Optional)\nRun this to delete all AWS resources and avoid charges",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "51e71657-53ff-403b-a3ee-dc28bdcf1188",
   "metadata": {},
   "outputs": [],
   "source": "# Cleanup AWS resources to avoid ongoing charges\n# This will delete the Aurora cluster, VPC, and all related resources\nimport subprocess\nimport sys\n\ndef cleanup_resources():\n    \"\"\"Execute cleanup script with proper completion handling\"\"\"\n    try:\n        print(\"üßπ Starting AWS resource cleanup...\")\n        print(\"‚ö†Ô∏è  This will delete Aurora cluster, VPC, and all related resources\")\n        \n        confirm = input(\"Type 'DELETE' to confirm resource cleanup: \")\n        if confirm != 'DELETE':\n            print(\"‚ùå Cleanup cancelled\")\n            return\n            \n        result = subprocess.run([sys.executable, 'clean.py'], \n                              capture_output=False, \n                              text=True, \n                              check=True)\n        print(\"‚úÖ Resource cleanup completed successfully!\")\n        \n    except subprocess.CalledProcessError as e:\n        print(f\"‚ùå Cleanup failed: {e}\")\n    except FileNotFoundError:\n        print(\"‚ùå clean.py not found. Please ensure the file exists in the current directory.\")\n        print(\"üí° You can run this manually in terminal: python clean.py\")\n    except KeyboardInterrupt:\n        print(\"‚ùå Cleanup interrupted by user\")\n\n# Uncomment the line below to run cleanup\n# cleanup_resources()\n\nprint(\"üí° Uncomment the line above to run resource cleanup\")\nprint(\"‚ö†Ô∏è  Ensure you want to delete all AWS resources before running\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}